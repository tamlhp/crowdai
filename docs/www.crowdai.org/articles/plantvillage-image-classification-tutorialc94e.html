<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.crowdai.org/articles/plantvillage-image-classification-tutorial?article_section_id=4 by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 23 Oct 2019 05:50:05 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <title>crowdAI</title>
    <meta name="description" content="Fighting for Open Science with Open Data">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Facebook Open Graph data -->
    <meta property="og:title" content="crowdAI" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="plantvillage-image-classification-tutorialc94e.html?article_section_id=4" />
    <meta property="og:image" content="../../d1u1amw606tzwl.cloudfront.net/assets/misc/crowdai-head-ea7fded0f7e7e11b446f4d80f8d663b2e60fdd06546dd72be81fd978422a0725.png" />
    <meta property="og:description" content="Fighting for Open Science with Open Data" />
    <meta property="og:site_name" content="crowdAI" />

    <!-- Twitter Card data -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@crowd_ai">
    <meta name="twitter:title" content="crowdAI">
    <meta name="twitter:description" content="Fighting for Open Science with Open Data">
    <meta name="twitter:creator" content="@crowd_ai">
    <meta name="twitter:image:src" content="../../d1u1amw606tzwl.cloudfront.net/assets/misc/crowdai-head-ea7fded0f7e7e11b446f4d80f8d663b2e60fdd06546dd72be81fd978422a0725.png">

    <link rel="apple-touch-icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/apple-icon-57x57-5f11dd362820389c669093ea0070a5a8f98e1128c84ab5c9316768989c488d9e.png" sizes="57x57" />
    <link rel="apple-touch-icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/apple-icon-60x60-a3ba860c24da457ef8f16d9d7e40814c6835bfef7268781eb17dfa1c03be83ed.png" sizes="60x60" />
    <link rel="apple-touch-icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/apple-icon-72x72-fa968c8656c46df290ed1ae5dd4bb65c429b015a661330a3623b096c6e4d253e.png" sizes="72x72" />
    <link rel="apple-touch-icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/apple-icon-76x76-5760f720632f080ba32a6afa52c2e48600d0fbb270f3a56d66947b6500cca3ba.png" sizes="76x76" />
    <link rel="apple-touch-icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/apple-icon-114x114-ffef2a7294e6e9c95b42e0e97c52103e350dd25a76094d73a107dacbb5580249.png" sizes="114x114" />
    <link rel="apple-touch-icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/apple-icon-120x120-aaddb625e759aeeaa326652d0e6ae80250ce6cc76e86cd14825af710cf7ff90f.png" sizes="120x120" />
    <link rel="apple-touch-icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/apple-icon-144x144-28f2bdfee1a11984ba0dd2da8e2fcd52fc6cd6350ccafb5512fa521f40f4de8b.png" sizes="144x144" />
    <link rel="apple-touch-icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/apple-icon-152x152-c42809322fa9f13067d094364700ca5b8ce238063dfc6d9d37609ba9428725c7.png" sizes="152x152" />
    <link rel="shortcut icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/apple-icon-180x180-6dcbd672f0ef87c1cd1b470dc1417ca1cac6c16e1443fbde7e03250f6deebada.png" sizes="192x192" />
    <link rel="shortcut icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/favicon-32x32-31fa8e51ed4c803d20eab56262d765c013deaefab71bdcdeaae2b83fb4461a9b.png" sizes="32x32" />
    <link rel="shortcut icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/favicon-96x96-7159cc97f7b1ef5b332d1384cf0fbf500c11886ed7bd743daab632d1aae80d79.png" sizes="96x96" />
    <link rel="shortcut icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/favicon-16x16-1526730d30e191cb84bd66a08d29d01165d2074733a361550afecc5edd2fd22e.png" sizes="16x16" />
    <link rel="apple-touch-icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/apple-icon-57x57-5f11dd362820389c669093ea0070a5a8f98e1128c84ab5c9316768989c488d9e.png" sizes="57x57" />
    <link rel="apple-touch-icon" type="image/png" href="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/apple-icon-57x57-5f11dd362820389c669093ea0070a5a8f98e1128c84ab5c9316768989c488d9e.png" sizes="57x57" />
    <link rel="manifest" href="https://d1u1amw606tzwl.cloudfront.net/assets/favicon/manifest-e95672d4553092f574d19ee4bf1d17197b734d61b67a30e5463c91fd875095d4.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="../../d1u1amw606tzwl.cloudfront.net/assets/favicon/ms-icon-144x144-28f2bdfee1a11984ba0dd2da8e2fcd52fc6cd6350ccafb5512fa521f40f4de8b.png">
    <meta name="theme-color" content="#ffffff">
    <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:700|Roboto:400,400i,500,500i,700" rel="stylesheet">
    <script src="../../use.fontawesome.com/bee91422da.js"></script>
        <script data-turbolinks-eval="false">
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      },
      i[r].l = 1 * new Date();
      a = s.createElement(o),
      m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '../../www.google-analytics.com/analytics.js', 'ga');
      ga('create', 'UA-75263739-1', 'auto', {'allowLinker': true });
    ga('require', 'linker');
    ga('linker:autoLink', ['crowdai.org']);
  </script>

      <script>
var _rollbarConfig = {
    accessToken: "56bdb20b34b040159a0f2c4e29eb39bb",
    captureUncaught: true,
    captureUnhandledRejections: true,
    payload: {
        environment: "production"
    }
};
// Rollbar Snippet
!function(r){function o(e){if(n[e])return n[e].exports;var t=n[e]={exports:{},id:e,loaded:!1};return r[e].call(t.exports,t,t.exports,o),t.loaded=!0,t.exports}var n={};return o.m=r,o.c=n,o.p="",o(0)}([function(r,o,n){"use strict";var e=n(1),t=n(4);_rollbarConfig=_rollbarConfig||{},_rollbarConfig.rollbarJsUrl=_rollbarConfig.rollbarJsUrl||"https://cdnjs.cloudflare.com/ajax/libs/rollbar.js/2.0.1/rollbar.min.js",_rollbarConfig.async=void 0===_rollbarConfig.async||_rollbarConfig.async;var a=e.setupShim(window,_rollbarConfig),l=t(_rollbarConfig);window.rollbar=e.Rollbar,a.loadFull(window,document,!_rollbarConfig.async,_rollbarConfig,l)},function(r,o,n){"use strict";function e(r){return function(){try{return r.apply(this,arguments)}catch(o){try{console.error("[Rollbar]: Internal error",o)}catch(n){}}}}function t(r,o){this.options=r,this._rollbarOldOnError=null;var n=s++;this.shimId=function(){return n},window&&window._rollbarShims&&(window._rollbarShims[n]={handler:o,messages:[]})}function a(r,o){var n=o.globalAlias||"Rollbar";if("object"==typeof r[n])return r[n];r._rollbarShims={},r._rollbarWrappedError=null;var t=new p(o);return e(function(){return o.captureUncaught&&(t._rollbarOldOnError=r.onerror,i.captureUncaughtExceptions(r,t),i.wrapGlobals(r,t)),o.captureUnhandledRejections&&i.captureUnhandledRejections(r,t),r[n]=t,t})()}function l(r){return e(function(){var o=this,n=Array.prototype.slice.call(arguments,0),e={shim:o,method:r,args:n,ts:new Date};window._rollbarShims[this.shimId()].messages.push(e)})}var i=n(2),s=0,c=n(3),d=function(r,o){return new t(r,o)},p=c.bind(null,d);t.prototype.loadFull=function(r,o,n,t,a){var l=function(){var o;if(void 0===r._rollbarDidLoad){o=new Error("rollbar.js did not load");for(var n,e,t,l,i=0;n=r._rollbarShims[i++];)for(n=n.messages||[];e=n.shift();)for(t=e.args||[],i=0;i<t.length;++i)if(l=t[i],"function"==typeof l){l(o);break}}"function"==typeof a&&a(o)},i=!1,s=o.createElement("script"),c=o.getElementsByTagName("script")[0],d=c.parentNode;s.crossOrigin="",s.src=t.rollbarJsUrl,n||(s.async=!0),s.onload=s.onreadystatechange=e(function(){if(!(i||this.readyState&&"loaded"!==this.readyState&&"complete"!==this.readyState)){s.onload=s.onreadystatechange=null;try{d.removeChild(s)}catch(r){}i=!0,l()}}),d.insertBefore(s,c)},t.prototype.wrap=function(r,o){try{var n;if(n="function"==typeof o?o:function(){return o||{}},"function"!=typeof r)return r;if(r._isWrap)return r;if(!r._wrapped){r._wrapped=function(){try{return r.apply(this,arguments)}catch(o){var e=o;throw"string"==typeof e&&(e=new String(e)),e._rollbarContext=n()||{},e._rollbarContext._wrappedSource=r.toString(),window._rollbarWrappedError=e,e}},r._wrapped._isWrap=!0;for(var e in r)r.hasOwnProperty(e)&&(r._wrapped[e]=r[e])}return r._wrapped}catch(t){return r}};for(var u="log,debug,info,warn,warning,error,critical,global,configure,handleUncaughtException,handleUnhandledRejection".split(","),f=0;f<u.length;++f)t.prototype[u[f]]=l(u[f]);r.exports={setupShim:a,Rollbar:p}},function(r,o){"use strict";function n(r,o){if(r){var n;"function"==typeof o._rollbarOldOnError?n=o._rollbarOldOnError:r.onerror&&!r.onerror.belongsToRollbar&&(n=r.onerror,o._rollbarOldOnError=n);var t=function(){var t=Array.prototype.slice.call(arguments,0);e(r,o,n,t)};t.belongsToRollbar=!0,r.onerror=t}}function e(r,o,n,e){r._rollbarWrappedError&&(e[4]||(e[4]=r._rollbarWrappedError),e[5]||(e[5]=r._rollbarWrappedError._rollbarContext),r._rollbarWrappedError=null),o.handleUncaughtException.apply(o,e),n&&n.apply(r,e)}function t(r,o){if(r){"function"==typeof r._rollbarURH&&r.removeEventListener("unhandledrejection",r._rollbarURH);var n=function(r){var n=r.reason,e=r.promise,t=r.detail;!n&&t&&(n=t.reason,e=t.promise),o&&o.handleUnhandledRejection&&o.handleUnhandledRejection(n,e)};r._rollbarURH=n,r.addEventListener("unhandledrejection",n)}}function a(r,o){if(r){var n,e,t="EventTarget,Window,Node,ApplicationCache,AudioTrackList,ChannelMergerNode,CryptoOperation,EventSource,FileReader,HTMLUnknownElement,IDBDatabase,IDBRequest,IDBTransaction,KeyOperation,MediaController,MessagePort,ModalWindow,Notification,SVGElementInstance,Screen,TextTrack,TextTrackCue,TextTrackList,WebSocket,WebSocketWorker,Worker,XMLHttpRequest,XMLHttpRequestEventTarget,XMLHttpRequestUpload".split(",");for(n=0;n<t.length;++n)e=t[n],r[e]&&r[e].prototype&&l(o,r[e].prototype)}}function l(r,o){if(o.hasOwnProperty&&o.hasOwnProperty("addEventListener")){var n=o.addEventListener;n._rollbarOldAdd&&(n=n._rollbarOldAdd);var e=function(o,e,t){n.call(this,o,r.wrap(e),t)};e._rollbarOldAdd=n,o.addEventListener=e;var t=o.removeEventListener;t._rollbarOldRemove&&(t=t._rollbarOldRemove);var a=function(r,o,n){t.call(this,r,o&&o._wrapped||o,n)};a._rollbarOldRemove=t,o.removeEventListener=a}}r.exports={captureUncaughtExceptions:n,captureUnhandledRejections:t,wrapGlobals:a}},function(r,o){"use strict";function n(r,o){this.impl=r(o,this),this.options=o,e(n.prototype)}function e(r){for(var o=function(r){return function(){var o=Array.prototype.slice.call(arguments,0);if(this.impl[r])return this.impl[r].apply(this.impl,o)}},n="log,debug,info,warn,warning,error,critical,global,configure,handleUncaughtException,handleUnhandledRejection,_createItem,wrap,loadFull,shimId".split(","),e=0;e<n.length;e++)r[n[e]]=o(n[e])}n.prototype._swapAndProcessMessages=function(r,o){this.impl=r(this.options);for(var n,e,t;n=o.shift();)e=n.method,t=n.args,this[e]&&"function"==typeof this[e]&&this[e].apply(this,t);return this},r.exports=n},function(r,o){"use strict";r.exports=function(r){return function(o){if(!o&&!window._rollbarInitialized){r=r||{};for(var n,e,t=r.globalAlias||"Rollbar",a=window.rollbar,l=function(r){return new a(r)},i=0;n=window._rollbarShims[i++];)e||(e=n.handler),n.handler._swapAndProcessMessages(l,n.messages);window[t]=e,window._rollbarInitialized=!0}}}}]);
// End Rollbar Snippet
</script>

    <link rel="stylesheet" media="all" href="../../d1u1amw606tzwl.cloudfront.net/assets/application-fbad2a77a3c16168d762d7bcd70b51933687bf62defed75590da7d06afb9c181.css" data-turbolinks-track="true" />
    <script src="../../d1u1amw606tzwl.cloudfront.net/assets/application-7d73590a8408508da67ef02bc6ff6e8dcdd86d359df9bb543dd538d7a9411554.js" data-turbolinks-track="true"></script>
    <meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="rvIcmqFCw02nsalFA7HawTvSHIiVZlKJS+QQ3xqhs/l/lefA74tQUXGS31Knk42YmI4jeP2vtM/0tfOFNd0zBA==" />
    <script src="../../d1u1amw606tzwl.cloudfront.net/packs/application-0a6ea7b74599e58741ff.js"></script>
    <link rel="stylesheet" media="screen" href="../../d1u1amw606tzwl.cloudfront.net/packs/application-a4f1ebb6e6bc4a2b9e31.css" />
  </head>

  <body id=''>
    <!-- loader container -->
  <div id='loader-container' class="loader-container">

    <!-- loader -->
    <div class="loader" title="0">
      <svg version="1.1" id="loader-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="40px" height="40px" viewBox="0 0 40 40" enable-background="new 0 0 40 40" xml:space="preserve">
        <path opacity="0.2" fill="#F0514D" d="M20.201,5.169c-8.254,0-14.946,6.692-14.946,14.946c0,8.255,6.692,14.946,14.946,14.946s14.946-6.691,14.946-14.946C35.146,11.861,28.455,5.169,20.201,5.169z M20.201,31.749c-6.425,0-11.634-5.208-11.634-11.634c0-6.425,5.209-11.634,11.634-11.634c6.425,0,11.633,5.209,11.633,11.634C31.834,26.541,26.626,31.749,20.201,31.749z"/>
        <path fill="#F0514D" d="M26.013,10.047l1.654-2.866c-2.198-1.272-4.743-2.012-7.466-2.012h0v3.312h0C22.32,8.481,24.301,9.057,26.013,10.047z">
        <animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 20 20" to="360 20 20" dur="0.5s" repeatCount="indefinite"/>
        </path>
      </svg>
      <br>
      <span>Loading</span>
    </div>
    <!-- loader -->

  </div>
  <!-- /loader container -->

    <div id='page-content'>
      <header>
  <div class="row">
    <div class="branding">
      <a class="logo" href="../index.html" title=""></a>
    </div>
    <nav class="primary">
      <ul>
        <li><a href="../challenges.html">Challenges</a></li>
        <li><a href="../articles.html">Knowledge Base</a></li>
        <li><a href="../job_postings.html">Job Board</a></li>
          <li><a href="../participants/sign_in.html">Log in</a></li>
      </ul>
    </nav>
    <div class="menu">
      <a id="mobile-nav" href="#">
        <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
      </a>
    </div>
  </div>
  <nav class="mobile-primary">
    <div id="mobile-nav-header">
      <div class="row">
        <a class="logo" href="#" title=""></a>
        <a id="mobile-nav" href="#">
          <i class="fa fa-times fa-lg" aria-hidden="true"></i>
        </a>
      </div>
    </div>
    <ul>
      <li><h3><a href="../challenges.html">Challenges</a></h3></li>
      <li><h3><a href="../articles.html">Knowledge Base</a></h3></li>
      <li><h3><a href="../job_postings.html">Job Board</a></h3></li>
    </ul>
      <ul>
        <li>
          <h4><a href="../participants/sign_in.html">Log in</a></h4>
        </li>
      </ul>
  </nav>
    <div class="cookies-eu js-cookies-eu">
    <span class="cookies-eu-content-holder">Cookies help us deliver our services. By using our services, you agree to our use of cookies.</span>
    <span class="cookies-eu-button-holder">
      <button class="btn btn-primary cookies-eu-ok js-cookies-eu-ok"> OK </button>
      <a href="../cookies.html" class=" btn cookies-eu-link" target="_blank"> Learn more </a>
    </span>
  </div>

</header>

      <div id='flash-messages'>
</div>

      <div class="site-message">
  <div class="row">
    <p>crowdAI is shutting down - please read our <a href="../blogs/7.html">blog post</a> for more information
  </div>
</div>

        <div class="content">
  <div class="masthead-detailed" data-cell='Article::Cell::ArticleMasthead'>
  <div class="row">
    <div class="pic pic-tutorial">
      <img src="../../dnczkxd1gcfu5.cloudfront.net/images/articles/image_file/1/1.Screen_Shot_2016-07-01_at_11.40.47.png" />
    </div>
    <div class="info info-tutorial">
      <div class="heading-w-icon">
        <h2>PlantVillage Image Classification Tutorial</h2>
      </div>
      <p class="large">Using AlexNet on Caffe to solve the PlantVillage image classification problem.</p>
      <div class="user-link">
          <a href="../participants/spmohanty.html">
            <img src="../../dnczkxd1gcfu5.cloudfront.net/images/participants/image_file/7/tt.png" />
</a>          <p>By
            <a href="../participants/spmohanty.html">spMohanty</a>
            &nbsp;
            04 July 2016
          </p>
      </div>
      <div class="button-group">
        <a id="vote-link-article-1" class="btn btn-secondary" href="../participants/sign_in.html"><i class='fa fa-heart' aria-hidden='true'></i>  21</a>
      </div>
    </div>
  </div>
</div>

  <div class="sub-nav">
  <div class="row">
    <ul>
          <li>
            <a href="plantvillage-image-classification-tutorial616b.html?article_section_id=1">Introduction</a>
            <sup>1</sup>
          </li>
          <li>
            <a href="plantvillage-image-classification-tutorial2b28.html?article_section_id=2">Approach</a>
            <sup>2</sup>
          </li>
          <li>
            <a href="plantvillage-image-classification-tutorial8fce.html?article_section_id=3">Setup</a>
            <sup>3</sup>
          </li>
          <li class="active">
            <a href="plantvillage-image-classification-tutorialc94e.html?article_section_id=4">Code</a>
            <sup>4</sup>
          </li>
    </ul>
  </div>
</div>

  <div class="row">
    <div class="article">
      <h2>Setting up the Model</h2>

<h3>Downloading pretrained model</h3>

<p>A pretrained AlexNet model along with the corresponding prototxt files for caffe are available at : <a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet">https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet</a></p>

<p>We download all the required files and store them in a separate folder named <code>AlexNet</code></p>
<pre class="highlight shell"><code>    <span class="nb">cd</span> /home/&lt;your_user_name&gt;/plantvillage
    mkdir AlexNet
    <span class="nb">cd </span>AlexNet
    wget http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel
    wget https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_alexnet/deploy.prototxt
    wget https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_alexnet/solver.prototxt
    wget https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_alexnet/train_val.prototxt<span class="sb">```</span>
</code></pre>

<h3>Updating LMDB data store references</h3>

<p>Now that we have all the required files, we will start with first pointing the <code>train_val.prototxt</code> to the correct training and validation lmdb stores (and also the corresponding mean file) . The <code>train_val.prototxt</code> file is the configuration file that caffe will refer to understand the structure of the network during the training and the validation phases.</p>

<p>We will do this, by editing the <code>train_val.prototxt</code> file to change the following block (<code>line 7-19</code>) :</p>
<pre class="highlight plaintext"><code>include {
    phase: TRAIN
}
transform_param {
    mirror: true
    crop_size: 227
    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
}
data_param {
    source: "examples/imagenet/ilsvrc12_train_lmdb"
    batch_size: 256
    backend: LMDB
}
</code></pre>

<p>to look like :</p>
<pre class="highlight plaintext"><code>include {
    phase: TRAIN
}
transform_param {
    mirror: true
    crop_size: 227
    mean_file: "../lmdb/mean.binaryproto"
}
data_param {
    source: "../lmdb/train_lmdb"
    batch_size: 256
    backend: LMDB
}
</code></pre>

<p>We will do the same thing for the prototxt block corresponding to the validation data, by changing the following block (<code>line 26-38</code>):</p>
<pre class="highlight plaintext"><code>include {
    phase: TEST
}
transform_param {
    mirror: false
    crop_size: 227
    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
}
data_param {
    source: "examples/imagenet/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
}
</code></pre>

<p>to look like :</p>
<pre class="highlight plaintext"><code>include {
    phase: TEST
}
transform_param {
    mirror: false
    crop_size: 227
    mean_file: "../lmdb/mean.binaryproto"
}
data_param {
    source: "../lmdb/val_lmdb"
    batch_size: 50
    backend: LMDB
}
</code></pre>

<h3>Adapting the downloaded AlexNet model for FineTuning on our dataset</h3>

<p>As the dataset we are working with needs to be classified across 38 classes instead of the standard 1000 classes that AlexNet was designed for; we will first change the number of outputs of the final layer from 1000 to 38. This can be done by manually editing the corresponding section for the last layer (<code>fc8</code> in this case) in both the <code>train_val.prototxt</code> and <code>deploy.prototxt</code>. The <code>num_output</code> value in that layer needs to be changed to 38 from 1000. And this can be quickly done by using :</p>
<pre class="highlight shell"><code>    <span class="nb">cd</span> /home/&lt;your_user_name&gt;/plantvillage/AlexNet
    sed -i -e <span class="s1">'s/num_output: 1000/num_output: 38/'</span> train_val.prototxt
    sed -i -e <span class="s1">'s/num_output: 1000/num_output: 38/'</span> deploy.prototxt<span class="sb">```</span>
</code></pre>

<p>Then we also need to reset the weights in the last layer of the network, which can be very easily done by renaming the last layer, so that Caffe has to re-initialize the weights of the layer when it does not find any corresponding weights in the associated layer. This can be done by manually renaming all references to <code>fc8</code> (the last layer) in both <code>train_val.prototxt</code> and <code>deploy.prototxt</code> to <code>fc8_plantvillage</code>. Or, it can be quickly done by using :</p>
<pre class="highlight shell"><code>    <span class="nb">cd</span> /home/&lt;your_user_name&gt;/plantvillage/AlexNet
    sed -i -e <span class="s1">'s/fc8/fc8_plantvillage/'</span> train_val.prototxt
    sed -i -e <span class="s1">'s/fc8/fc8_plantvillage/'</span> deploy.prototxt
</code></pre>

<h3>Configuring the Solver Parameters</h3>

<p>In the final step before we can start training, we need to configure the solver parameters in <code>solver.prototxt</code>. To start with, we should start with a base learning rate of <code>0.001</code>. The reason being, as we will be finetuning an already trained model, the model is in principle much ahead in the training phase in contrast to when you try to start training from scratch. Then we will start experimenting by running the training for <code>30 epochs</code>, where 1 epoch basically refers to one full pass through the training set.</p>

<p>The final <code>solver.prototxt</code> should look like :</p>
<pre class="highlight plaintext"><code>net: "train_val.prototxt"
test_iter: 3
test_interval: 59
base_lr: 0.001
lr_policy: "step"
gamma: 0.1
stepsize: 590
display: 11
max_iter: 1770
momentum: 0.9
weight_decay: 0.0005
snapshot: 59
snapshot_prefix: "../snapshots/snapshots_"
solver_mode: GPU
</code></pre>

<p><strong>NOTE:</strong> The <code>solver_mode</code> parameter should be set to <code>CPU</code> if you do not have access to a GPU on the host machine. Apart from that, these parameters are mostly hyperparameters which you will need to hand tune a bit till you are confident you get the best results.</p>

<p>we will also need to create a folder called as <code>snapshots</code> where caffe can dump the models at certain intervals. Based on the reference we gave in our <code>solver.prototxt</code>, we will have to create it at : <code>/home/&lt;your_user_name&gt;/plantvillage/snapshots</code>, so we should simply do a :</p>
<pre class="highlight plaintext"><code> mkdir /home/&lt;your_user_name&gt;/plantvillage/snapshots
</code></pre>

<h2>Training</h2>

<p>If you followed all the previous steps correctly, then you should be able to start the training simply by :</p>
<pre class="highlight plaintext"><code>cd /home/&lt;your_user_name&gt;/plantvillage/AlexNet
$CAFFE_ROOT/build/tools/caffe train \
    -solver solver.prototxt \
    -weights bvlc_alexnet.caffemodel
    -gpu 0 #Only if you have a GPU, else you should ignore this flag```
</code></pre>

<p>If you are running the training in GPU mode, and you get an <code>out of memory</code> error, then you can try reducing the training and testing <code>batch_size</code> in <code>train_val.prototxt</code> in line_number <code>17</code> and <code>36</code>.</p>

<h2>Prediction</h2>

<p>The first step is to select the model that we will use to predict. In the <code>snapshots</code> folder, you will find many files of the form <code>snapshots__&lt;iteration_number&gt;.caffemodel</code> or <code>snapshots__&lt;iteration_number&gt;.solverstate</code>. The <code>*.solverstate</code> files are used to "resume" the training from a particular state, while the <code>*.caffemodel</code> files is primarily used for prediction, but it is pretty much the same as the solverstate file minus some training specific state variables. So we will select the latest model from among the snapshots, and use that in the following sub section for prediction all the test images. You can do that by :</p>
<pre class="highlight plaintext"><code>cd /home/&lt;your_user_name&gt;/plantvillage/
cp snapshots/`ls -t snapshots/ | head -n 1` plantvillage.caffemodel`
</code></pre>

<p>Please note that, if you want to use the snapshot from any other iteration that the latest iteration, you can also do something like <code>cp snapshots/snapshots__&lt;iteration_number&gt;.caffemodel plantvillage.caffemodel</code></p>

<p>And now we move to the actual predictions of all the test images.<br>
Before we can predict the class of the images, we will have to first make them similar to the kind of images we used for training. Before we initiated the training, we "squashed" the images to <code>256x256 pixels</code>, and now we do the same to all the test images. You can do that by creating the following script at <code>/home/&lt;your_user_name&gt;/plantvillage/resize_test_images.sh</code>:</p>
<pre class="highlight shell"><code>    <span class="c">#!/bin/bash</span>

    <span class="nv">TEST_FOLDER_NAME</span><span class="o">=</span><span class="s2">"/home/&lt;your_user_name&gt;/plantvillage/test"</span>
    <span class="k">for </span>file <span class="k">in</span> <span class="sb">`</span>ls <span class="nv">$TEST_FOLDER_NAME</span><span class="sb">`</span>
    <span class="k">do
        </span><span class="nb">echo</span> <span class="nv">$file</span>
        convert <span class="nv">$TEST_FOLDER_NAME</span>/<span class="nv">$file</span> -resize 256x256! <span class="nv">$TEST_FOLDER_NAME</span>/<span class="nv">$file</span>
    <span class="k">done</span>
</code></pre>

<p>Then you can execute it by :</p>
<pre class="highlight shell"><code>    <span class="nb">cd</span> /home/&lt;your_user_name&gt;/plantvillage
    chmod +x resize_test_images.sh
    ./resize_test_images.sh<span class="sb">```</span>
</code></pre>

<p>Now that we have everything in order, we can use the following script to get started on how to make the predictions and put it in an appropriate format for the crowdAI PlantVillage Classification Challenge. This script has to exist at the location <code>/home/&lt;your_user_name&gt;/plantvillage/predict.py</code>.</p>
<pre class="highlight python"><code>    <span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
    <span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="kn">import</span> <span class="nn">glob</span>

    <span class="s">"""
    # You can use the commented block of code below to
    #  make sure that caffe is on the python path:
    # This takes the path to caffe_root from the environment variable, so make sure
    # the $CAFFE_ROOT environment variable is set
    #
    #
    caffe_root = os.environ['CAFFE_ROOT']
    import sys
    sys.path.insert(0, caffe_root + 'python')
    """</span>

    <span class="kn">import</span> <span class="nn">caffe</span>

    <span class="s">"""
    # Adapted from from : http://www.cc.gatech.edu/~zk15/deep_learning/classify_test.py
    """</span>

    <span class="c"># Set the right path to your model definition file, pretrained model weights,</span>
    <span class="c"># and the image you would like to classify.   </span>
    <span class="n">MODEL_FILE</span> <span class="o">=</span> <span class="s">'AlexNet/deploy.prototxt'</span>
    <span class="n">PRETRAINED</span> <span class="o">=</span> <span class="s">'plantvillage.caffemodel'</span>
    <span class="n">BINARY_PROTO_MEAN_FILE</span> <span class="o">=</span> <span class="s">"lmdb/mean.binaryproto"</span>

    <span class="s">"""
    # Replicated from https://github.com/BVLC/caffe/issues/290
    """</span>
    <span class="n">blob</span> <span class="o">=</span> <span class="n">caffe</span><span class="o">.</span><span class="n">proto</span><span class="o">.</span><span class="n">caffe_pb2</span><span class="o">.</span><span class="n">BlobProto</span><span class="p">()</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span> <span class="n">BINARY_PROTO_MEAN_FILE</span>  <span class="p">,</span> <span class="s">'rb'</span> <span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="n">blob</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">mean_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="n">caffe</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">blobproto_to_array</span><span class="p">(</span><span class="n">blob</span><span class="p">)</span> <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>


    <span class="c">##NOTE : If you do not have a GPU, you can uncomment the `set_mode_cpu()` call</span>
    <span class="c">#        instead of the `set_mode_gpu` call.</span>
    <span class="n">caffe</span><span class="o">.</span><span class="n">set_mode_gpu</span><span class="p">()</span>
    <span class="c">#caffe.set_mode_cpu()</span>


    <span class="n">net</span> <span class="o">=</span> <span class="n">caffe</span><span class="o">.</span><span class="n">Classifier</span><span class="p">(</span><span class="n">MODEL_FILE</span><span class="p">,</span> <span class="n">PRETRAINED</span><span class="p">,</span>
                       <span class="n">mean</span><span class="o">=</span><span class="n">mean_arr</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
                       <span class="n">channel_swap</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span>
                       <span class="n">raw_scale</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                       <span class="n">image_dims</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">"output.csv"</span><span class="p">,</span> <span class="s">"w"</span><span class="p">)</span>
     <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">"filename,c_0,c_1,c_2,c_3,c_4,c_5,c_6,c_7,c_8,c_9,c_10,c_11,c_12,c_13,c_14,c_15,c_16,c_17,c_18,c_19,c_20,c_21,c_22,c_23,c_24,c_25,c_26,c_27,c_28,c_29,c_30,c_31,c_32,c_33,c_34,c_35,c_36,c_37</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>

    <span class="n">number_of_files_processed</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_file</span> <span class="ow">in</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s">"./test/*"</span><span class="p">):</span>
         <span class="n">number_of_files_processed</span> <span class="o">+=</span> <span class="mi">1</span>
         <span class="n">FileName</span> <span class="o">=</span> <span class="n">_file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">"/"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
         <span class="n">input_image</span> <span class="o">=</span> <span class="n">caffe</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">load_image</span><span class="p">(</span><span class="n">_file</span><span class="p">)</span>
         <span class="n">prediction</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">input_image</span><span class="p">])</span>
         <span class="n">s</span> <span class="o">=</span> <span class="n">FileName</span><span class="o">+</span><span class="s">","</span>
         <span class="k">for</span> <span class="n">probability</span> <span class="ow">in</span> <span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
             <span class="n">s</span><span class="o">+=</span><span class="nb">str</span><span class="p">(</span><span class="n">probability</span><span class="p">)</span><span class="o">+</span><span class="s">","</span>
         <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span>
         <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
         <span class="k">print</span> <span class="s">"Number of files : "</span><span class="p">,</span> <span class="n">number_of_files_processed</span>
         <span class="k">print</span> <span class="s">'predicted class:'</span><span class="p">,</span> <span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
         <span class="k">print</span> <span class="s">"**********************************************"</span>
</code></pre>

<p>and then execute it by</p>
<pre class="highlight shell"><code>    <span class="nb">cd</span> /home/&lt;your_user_name&gt;/plantvillage
    python predict.py
</code></pre>

<p>Finally after this script executes successfully (this will take some time ;) So, be patient !! ), you should have a <code>output.csv</code> in the format that the CrowdAI PlantVillage Classification Challenge expects. <strong>But please note that you will be disqualified if you use this approach to make a submission, as FineTuning / Transfer Learning based approaches are not allowed according to the rules of the challenge.</strong></p>

    </div>
  </div>
</div>


      
<div class="js-paloma-hook" data-palomaid="1571809203587">
  <script type="text/javascript">
    (function(){

      if ( !window['Paloma'] ) return true;
      Paloma.env = 'production';

      var id = "1571809203587",
          request = {"resource":"Articles","action":"show","params":{}};

      Paloma.engine.setRequest({
        id: id,
        resource: request.resource,
        action: request.action,
        params: request.params
      });

      var self  = document.querySelector("[data-palomaid='" + id + "']");
      if (self) self.parentNode.removeChild(self);

    })();
  </script>
</div>

      <footer class="">
  <div class="row">
    <a class="logo-footer" href="http://epfl.ch/">
      <img src="../../d1u1amw606tzwl.cloudfront.net/assets/misc/EPFL_Logo_Digital_RGB_PROD-60bb226e14e8a37db07793a88a44ce5fe4dfa7f23bc8bab21a52c8664e26b8ab.png" />
    </a>
    <p class="copyright">&#9400; 2018 crowdAI</p>
    <ul class="social">
      <li>
        <a href="https://twitter.com/crowd_ai">
          <i class="fa fa-twitter fa-lg" aria-hidden="true"></i>
        </a>
      </li>
      <li>
        <a href="https://github.com/crowdAI">
          <i class="fa fa-github fa-lg" aria-hidden="true"></i>
        </a>
      </li>
    </ul>
    <ul class="links">
      <li><a href="../faq.html">FAQ</a></li>
      <li><a href="../contact.html">Contact</a></li>
      <li><a href="../privacy.html">Privacy</a></li>
      <li><a href="../terms.html">Terms of Service</a></li>
      <li><a href="../cookies.html">Cookie consent</a></li>
    </ul>
  </div>
</footer>

    </div>
      <script>
    ga('set', 'location', location.href.split('#')[0]);
    ga('send', 'pageview', {"title": document.title});
  </script>

  </body>

<!-- Mirrored from www.crowdai.org/articles/plantvillage-image-classification-tutorial?article_section_id=4 by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 23 Oct 2019 05:50:05 GMT -->
</html>
